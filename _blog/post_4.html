---
title: "Backprop in Neural Networks"
collection: blog
permalink: /blog/post_4
excerpt: 'Step by step derivation of Backpropagation Algorithm in Neural Networks'
date: 2017-03-01
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>

<body>
  
<p>This article serves as a good exercise to see how forward propagation works and then how the gradients are computed to implement the backpropagation algorithm.
Also, the reader will get comfortable with the computation of vector, tensor derivatives and vector/matrix calculus.
A useful document can be found <a href="https://compsci697l.github.io/docs/vecDerivs.pdf">here</a> for the interested reader to get familiar with tensor operations.</p>

  
<p>Compute the gradients and derivatives of the loss function of the given neural network as shown in the Fig 1.
with respect to the parameters: <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>.
Where <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span> are the weight matrices;
and <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> are bias the vectors.
Let <span class="math inline">\(x \in \mathcal{R}^2\)</span>, <span class="math inline">\(W_{1} \in \mathcal{R}^{2*500}\)</span>, <span class="math inline">\(b_{1} \in \mathcal{R}^{500}\)</span>, <span class="math inline">\(W_{2} \in \mathcal{R}^{500*2}\)</span> and <span class="math inline">\(b_{2} \in \mathcal{R}^2\)</span>.
Also, show how the forward and backpropagation algorithms work.</p>
 
<figure>
<figcaption>Fig 1. Simple Feedforward Neural Network</figcaption>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/nn-3-layer-network.png?raw=true" alt="Simple Feedforward Neural Network" width="600" height="600" align="middle">
</figure>
  
<p>Let us first compute the forward propagation. Let <em>x</em> be the input.
The first hidden layer is computed as follows: <span class="math display">\[z_{1} = \textit{x}W_{1} + b_{1}
\label{eqn:one}\]</span></p>
  
<p>We then apply a non-linear activation function to equation <span class="math display">\[a_{1} = \textit{tanh}(z_{1})
\label{eqn:two}\]</span></p>
<p>The output layers’ activation are obtained using the following transformation <span class="math display">\[z_{2} = a_{1}W_{2} + b_{2}
\label{eqn:three}\]</span></p>
<p>Finally, a <em>softmax</em> is applied to get:</p>
<p><span class="math display">\[a_{2} = \hat{y} = softmax(z_{2})
\label{eqn:four}\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> is the predicted output by the feedforward network</p>

  
 <p>Let’s see this feedforward network through a circuit diagram as illustrated in Fig 2.
 Now, let’s see how the derivatives are computed with respect to the hidden nodes and bias vectors. Refer Fig 3.</p>

<figure>
<figcaption>Fig 2. Feedforward Circuit Diagram</figcaption>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/forward.png?raw=true" alt="Feedforward Circuit Diagram" width="600" height="600" align="middle">
</figure>
  
<figure>
<figcaption>Fig 3. Backpropagation Circuit Diagram</figcaption>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/backward.png?raw=true" alt="Backpropagation Circuit Diagram" width="600" height="600" align="middle">
</figure>
  
 <p>We have to compute the derivative of the loss function with respect to <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> that is see the effect of these parameters on the loss function (which we actually have to minimize).</p>
 
<p>Below are the steps to compute the various gradients as shown in figure:</p>
<p><span class="math display">\[Loss = y * ln(\sigma(z_{2})) + (1-y) * ln(1-\sigma(z_{2}))
\label{eqn:five}\]</span></p>
<p>Also, note that:</p>
<p><span class="math display">\[\frac{d\sigma(z_{2})}{dz} = ln(\sigma(z_{2})) * ln(1-\sigma(z_{2}))
\label{eqn:six}\]</span></p>
<p>Therefore, <span class="math display">\[\frac{\partial Loss}{\partial z_{2}} = y - \sigma(z_{2}) = y - \hat{y}
\label{eqn:seven}\]</span></p>
<p>Since, <span class="math inline">\(\sigma(z_{2})\)</span> = <span class="math inline">\(\hat{y}\)</span></p>
  
<p><span class="math display">\[\frac{\partial z_{2}}{\partial w_{2}} = \frac{\partial (a_{1}W_{2}+b_{2})}{\partial w_{2}} = a_{1}
\label{eqn:eight}\]</span></p>
<p><span class="math display">\[\frac{\partial z_{2}}{\partial b_{2}} = 1
\label{eqn:nine}\]</span></p>
<p><span class="math display">\[\frac{\partial z_{2}}{\partial tanh(z_{1})} = \frac{\partial(a_{1}W_{2}+b_{2})} {\partial tanh(z_{1})} = \frac{\partial(tanh(z_{1})W_{2}+b_{2})} {\partial tanh(z_{1})} = z_{2}
\label{eqn:ten}\]</span></p>
<p><span class="math display">\[\frac{\partial tanh(z_{1})}{\partial z_{1}} = 1 - tanh^2(z_{1})
\label{eqn:eleven}\]</span></p>

<p><span class="math display">\[\frac{\partial z_{1}}{\partial W_{1}} = \frac{\partial (xW_{1}+b_{1})}{\partial W_{1}} = x
\label{eqn:thirteen}\]</span></p>
<p><span class="math display">\[\frac{\partial z_{1}}{\partial b_{1}} = \frac{\partial (xW_{1}+b_{1})}{\partial b_{1}} = 1
\label{eqn:fourteen}\]</span></p>
  
  
<p>Finally, we can now use the chain rule to compute the effect of the four parameters namely <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> on the Loss function.</p>
<p>In what follows, <span class="math inline">\((P)^T\)</span> indicates the transpose of some matrix or vector P.</p>
  
  
<p><span class="math display">\[\frac{\partial Loss}{\partial W_{2}} = \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial W_{2}} = a_{1}^T(y-\hat{y})
\label{eqn:fifteen}\]</span></p>
  
<p><span class="math display">\[\frac{\partial Loss}{\partial b_{2}} = \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial b_{2}} = (y-\hat{y})
\label{eqn:sixteen}\]</span></p>

<p><span class="math display">\[\begin{aligned}
\frac{\partial Loss}{\partial W_{1}} &amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2} }{\partial tanh(z_{1})} * \frac{\partial tanh(z_{1})}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= x^T (1-tanh^2Z_{1}) (y-\hat{y}) W_{2}^T
\label{eqn:seventeen}\end{aligned}\]</span></p>
  
<p><span class="math display">\[\begin{aligned}
\frac{\partial Loss}{\partial b_{1}} &amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2} }{\partial tanh(z_{1})} * \frac{\partial tanh(z_{1})}{\partial z_{1}} * \frac{\partial z_{1}}{\partial b_{1}}\\
&amp;= (1-tanh^2Z_{1}) (y-\hat{y}) W_{2}^T
\label{eqn:eighteen}\end{aligned}\]</span></p>
  
 <p>Hence, we have computed both the forward propagation and back propagation for the given multi-layer neural network.</p>
 
  
  
</body>
</html>
