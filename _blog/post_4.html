---
title: "Backprop in Neural Networks"
collection: blog
permalink: /blog/post_4
excerpt: 'Step by step derivation of Backpropagation Algorithm in Neural Networks'
date: 2017-03-01
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>

<body>
  
<p>This question serves as good exercise to see how forward propagation works and then how the gradients are computed to implement the backpropagation algorithm.
Also, the reader will get comfortable with the computation of vector, tensor derivatives and vector/matrix calculus.
A useful document can be found <a href="https://compsci697l.github.io/docs/vecDerivs.pdf">here</a> for the interested reader to get familiar with tensor operations.</p>

  
<p>Compute the gradients and derivatives of the loss function of the given neural network as shown in the figure below
with respect to the parameters: <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>.
Where <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span> are the weight matrices;
and <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> are bias the vectors.
Let <span class="math inline">\(x \in \mathcal{R}^2\)</span>, <span class="math inline">\(W_{1} \in \mathcal{R}^{2*500}\)</span>, <span class="math inline">\(b_{1} \in \mathcal{R}^{500}\)</span>, <span class="math inline">\(W_{2} \in \mathcal{R}^{500*2}\)</span> and <span class="math inline">\(b_{2} \in \mathcal{R}^2\)</span>.
Also, show how the forward and backpropagation algorithms work.</p>
 
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/nn-3-layer-network.png?raw=true" alt="Simple Feedforward Network" width="600" height="600" align="middle">

<p>Let us first compute the forward propagation. Let <em>x</em> be the input.
The first hidden layer is computed as follows: <span class="math display">\[z_{1} = \textit{x}W_{1} + b_{1}
\label{eqn:one}\]</span></p>
  
<p>We then apply a non-linear activation function to equation <a href="#eqn:one" data-reference-type="ref" data-reference="eqn:one">[eqn:one]</a> <span class="math display">\[a_{1} = \textit{tanh}(z_{1})
\label{eqn:two}\]</span></p>
<p>The output layers’ activation are obtained using the following transformation <span class="math display">\[z_{2} = a_{1}W_{2} + b_{2}
\label{eqn:three}\]</span></p>
<p>Finally, a <em>softmax</em> is applied to equation <a href="#eqn:two" data-reference-type="ref" data-reference="eqn:two">[eqn:two]</a> to get:</p>
<p><span class="math display">\[a_{2} = \hat{y} = softmax(z_{2})
\label{eqn:four}\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> is the predicted output by the feedforward network</p>

  
 <p>Let’s see this feedforward network through a circuit diagram as illustrated in figure .
 Now, let’s see how the derivatives are computed with respect to the hidden nodes and bias vectors. Refer figure .</p>

</body>
</html>
