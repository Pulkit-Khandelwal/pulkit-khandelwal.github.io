---
title: "Backprop in Neural Networks"
collection: blog
permalink: /blog/post_4
excerpt: 'Backprop in Neural Networks'
date: 2017-03-01
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
<p><span class="math display">\[\frac{\partial Loss}{\partial b_{2}} = \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial b_{2}} = (y-\hat{y})
\label{eqn:sixteen}\]</span></p>
<p>It is a good practice to use gradient checking to compare the analytic and numerical gradient. Oftentimes, we see the absolute error i.e. during the iterations of the gradient descent algorithm the absolute value of the difference between the consecutive gradients are checked. The algorithm terminates when the absolute error is less than some small threshold.<br />
But, using absolute error is not recommended and rather the use of relative error is suggested.<br />
Prove this statement with a numerical example.</p>
<h2 id="solution">Solution</h2>
<p>Let <span class="math inline">\(g_{n}\)</span> and <span class="math inline">\(g_{n+1}\)</span> be the two consecutive numerical gradients.<br />
<span class="math inline">\(|g_{n}\)</span> - <span class="math inline">\(g_{n+1}|\)</span> is the absolute error which is calculated and if the error is less than some threshold then the gradient check is successful.<br />
If <span class="math inline">\(g_{n}\)</span> and <span class="math inline">\(g_{n+1}\)</span> are both very close to 1. For example <span class="math inline">\(g_{n}\)</span>= 1 and <span class="math inline">\(g_{n+1}\)</span> = 1.0001 then<br />
<span class="math inline">\(|g_{n}\)</span> - <span class="math inline">\(g_{n+1}|\)</span> is of the order <span class="math inline">\(10^{-4}\)</span><br />
and the gradient check looks good.<br />
But, when <span class="math inline">\(g_{n}\)</span> and <span class="math inline">\(g_{n+1}\)</span> both have very low values in the order of <span class="math inline">\(10^{-6}\)</span>; then an absolute error of <span class="math inline">\(10^{-4}\)</span> will be considered very large and hence gradient check will fail.<br />
Therefore, it is better to use the relative error defined as <span class="math inline">\(\frac{|g_{n} - g_{n+1}|}{|g_{n} + g_{n+1}|}\)</span><br />
This is the ratio between the absolute difference to the absolute sum of the two gradients. This method thus scales the error and gives us the correct error estimate than when only the absolute error is used.<br />
Instability can be encountered when performing the gradient check. Here, I ask one more question which seems interesting.<br />
<strong><em>Question</em></strong><br /></p>
<figure>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/post_3_img_1.png" alt="Neural Network" style="width:50.0%" /><figcaption>Neural Network<span label="fig:a"></span></figcaption>
</figure>
</body>
</html>
