---
title: "Backprop in Neural Networks"
collection: blog
permalink: /blog/post_4
excerpt: 'Backprop in Neural Networks'
date: 2017-03-01
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
  
<p>Compute the gradients and derivatives of the loss function of the given neural network as shown in figure <a href="#fig:a" data-reference-type="ref" data-reference="fig:a">[fig:a]</a> with respect to the parameters: <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>. Where <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span> are the weight matrices; and <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> are bias the vectors. Let <span class="math inline">\(x \in \mathcal{R}^2\)</span>, <span class="math inline">\(W_{1} \in \mathcal{R}^{2*500}\)</span>, <span class="math inline">\(b_{1} \in \mathcal{R}^{500}\)</span>, <span class="math inline">\(W_{2} \in \mathcal{R}^{500*2}\)</span> and <span class="math inline">\(b_{2} \in \mathcal{R}^2\)</span>. Also, show how the forward and backpropagation algorithms work.</p>


</body>
</html>
