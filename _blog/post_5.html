---
title: "Generate label footprints for Satellite Imagery"
collection: blog
permalink: /blog/post_5
excerpt: 'Given an AOI of a satellite image, can we get the corresponding groundtruth labels with a some python code?'
date: 2018-02-19
tags:
  - GIS
  - Satellite Imagery
  - machine learning
  - Open Street Maps
  - Python
  - API
  - geoPandas
  - gdal
  - OSM
  - Bokeh
  - geoJSON
  
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>MathJax example</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });  </script>
  
<style>
figure {
  text-align: center;
  font-style: italic;
  font-size: smaller;
  text-indent: 0;
  border: thin silver solid;
  margin: 0.5em;
  padding: 0.5em;
}
</style>
</head>

<body>
  
<p>Suppose you have satellite imagery for an AOI. You can do all sorts of analysis. But, with the advent of deep learning you want to 
  to some really cool stuff. You want to get the footprints of all the buildings in the Le Plateau area of Montreal or all the 
  roads in SoMA in San Francisco or perhaps you want to see where are all the general amentities in the Italian quarter in Boston. </p>
  
<p>We will compute the gradients and derivatives of the loss function of the given neural network as shown in the Fig 1.
with respect to the parameters: <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span>.
Where <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span> are the weight matrices;
and <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> are bias the vectors.
Let <span class="math inline">\(x \in \mathcal{R}^2\)</span>, <span class="math inline">\(W_{1} \in \mathcal{R}^{2*500}\)</span>, <span class="math inline">\(b_{1} \in \mathcal{R}^{500}\)</span>, <span class="math inline">\(W_{2} \in \mathcal{R}^{500*2}\)</span> and <span class="math inline">\(b_{2} \in \mathcal{R}^2\)</span>.
Also, show how the forward and backpropagation algorithms work.</p>
 
<figure>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/nn-3-layer-network.png?raw=true" alt="Simple Feedforward Neural Network" width="600" height="600" align="middle">
<figcaption>Fig 1. Simple Feedforward Neural Network</figcaption>
</figure>
  
<p>Let us first compute the forward propagation. Let <em>x</em> be the input.
The first hidden layer is computed as follows: <span class="math display">\[z_{1} = \textit{x}W_{1} + b_{1}
\tag{1}\]</span></p>
  
<p>We then apply a non-linear activation function to equation <span class="math display">\[a_{1} = \textit{tanh}(z_{1})
\tag{2}\]</span></p>
<p>The output layers’ activation are obtained using the following transformation <span class="math display">\[z_{2} = a_{1}W_{2} + b_{2}
\tag{3}\]</span></p>
<p>Finally, a <em>softmax</em> is applied to get:</p>
<p><span class="math display">\[a_{2} = \hat{y} = softmax(z_{2})
\tag{4}\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> is the predicted output by the feedforward network</p>

  
 <p>Let’s see this feedforward network through a circuit diagram as illustrated in Fig 2.
 Now, let’s see how the derivatives are computed with respect to the hidden nodes and bias vectors. Refer Fig 3.</p>

<figure>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/forward.png?raw=true" alt="Feedforward Circuit Diagram" width="600" height="600" align="middle">
<figcaption>Fig 2. Feedforward Circuit Diagram</figcaption>
</figure>
  
<figure>
<img src="https://github.com/Pulkit-Khandelwal/pulkit-khandelwal.github.io/blob/master/_blog/backward.png?raw=true" alt="Backpropagation Circuit Diagram" width="600" height="600" align="middle">
<figcaption>Fig 3. Backpropagation Circuit Diagram</figcaption>
</figure>
  
<p>We have to compute the derivative of the loss function with respect to <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> that is see the effect of these parameters on the loss function (which we actually have to minimize).</p>
 
<p>Below are the steps to compute the various gradients as shown in figure:</p>
<p><span class="math display">\[Loss = y * ln(\sigma(z_{2})) + (1-y) * ln(1-\sigma(z_{2}))
\tag{5}\]</span></p>
<p>Also, note that:</p>
<p><span class="math display">\[\frac{d\sigma(z_{2})}{dz} = ln(\sigma(z_{2})) * ln(1-\sigma(z_{2}))
\tag{6}\]</span></p>
<p>Therefore, <span class="math display">\[\frac{\partial Loss}{\partial z_{2}} = y - \sigma(z_{2}) = y - \hat{y}
\tag{7}\]</span></p>
<p>Since, <span class="math inline">\(\sigma(z_{2})\)</span> = <span class="math inline">\(\hat{y}\)</span></p>
  
<p><span class="math display">\[\frac{\partial z_{2}}{\partial w_{2}} = \frac{\partial (a_{1}W_{2}+b_{2})}{\partial w_{2}} = a_{1}
\tag{8}\]</span></p>
<p><span class="math display">\[\frac{\partial z_{2}}{\partial b_{2}} = 1
\tag{9}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\frac{\partial z_{2}}{\partial tanh(z_{1})} &amp;= \frac{\partial(a_{1}W_{2}+b_{2})} {\partial tanh(z_{1})}\\
&amp;= \frac{\partial(tanh(z_{1})W_{2}+b_{2})} {\partial tanh(z_{1})}\\
&amp;= z_{2}
\end{aligned}\tag{10}\]</span></p>
<p><span class="math display">\[\frac{\partial tanh(z_{1})}{\partial z_{1}} = 1 - tanh^2(z_{1})
\tag{11}\]</span></p>

<p><span class="math display">\[\frac{\partial z_{1}}{\partial W_{1}} = \frac{\partial (xW_{1}+b_{1})}{\partial W_{1}} = x
\tag{12}\]</span></p>
<p><span class="math display">\[\frac{\partial z_{1}}{\partial b_{1}} = \frac{\partial (xW_{1}+b_{1})}{\partial b_{1}} = 1
\tag{13}\]</span></p>
  
<p>Finally, we can now use the chain rule to compute the effect of the four parameters namely <span class="math inline">\(W_{1}\)</span>, <span class="math inline">\(W_{2}\)</span>, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> on the Loss function.</p>
<p>In what follows, <span class="math inline">\((P)^T\)</span> indicates the transpose of some matrix or vector P.</p>
  
<p><span class="math display">\[\frac{\partial Loss}{\partial W_{2}} = \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial W_{2}} = a_{1}^T(y-\hat{y})
\tag{14}\]</span></p>
  
<p><span class="math display">\[\frac{\partial Loss}{\partial b_{2}} = \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial b_{2}} = (y-\hat{y})
\tag{15}\]</span></p>

<p><span class="math display">\[\begin{aligned}
\frac{\partial Loss}{\partial W_{1}} &amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2} }{\partial tanh(z_{1})} * \frac{\partial tanh(z_{1})}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= x^T (1-tanh^2Z_{1}) (y-\hat{y}) W_{2}^T
\end{aligned}\tag{16}\]</span></p>
  
<p><span class="math display">\[\begin{aligned}
\frac{\partial Loss}{\partial b_{1}} &amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2}}{\partial z_{1}} * \frac{\partial z_{1}}{\partial W_{1}} \\
&amp;= \frac{\partial Loss}{\partial z_{2}} * \frac{\partial z_{2} }{\partial tanh(z_{1})} * \frac{\partial tanh(z_{1})}{\partial z_{1}} * \frac{\partial z_{1}}{\partial b_{1}}\\
&amp;= (1-tanh^2Z_{1}) (y-\hat{y}) W_{2}^T
\end{aligned}\tag{17}\]</span></p>
  
 <p>Hence, we have computed both the forward propagation and back propagation for the given multi-layer neural network.</p>
 
  
  
</body>
</html>
